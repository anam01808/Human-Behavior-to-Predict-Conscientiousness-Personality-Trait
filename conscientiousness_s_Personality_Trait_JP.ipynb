{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fVmsLZpuMP5"
      },
      "source": [
        "### ***Link To drive***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6grUi1j_W1sv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XCPyA6uuYUc"
      },
      "source": [
        "### ***Import Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvMGkMPwyieZ"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw-NugY2ZstK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lz63TAcufbN"
      },
      "source": [
        "### ***Load Dataset***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpNLiYu4uemU"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/mbti_1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyLE0nW7ujo4"
      },
      "source": [
        "### ***Preprocessing***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7quBrG1FwTBk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "import nltk # Make sure to import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to preprocess text with lemmatization and stemming\n",
        "def preprocess_text_full(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove special characters, digits, and punctuations\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    # Stemming\n",
        "    stemmed = [stemmer.stem(word) for word in lemmatized]\n",
        "    # Join tokens back into string\n",
        "    return ' '.join(stemmed)\n",
        "\n",
        "# Apply full preprocessing to the 'text' column\n",
        "df['full_preprocessed_text'] = df['posts'].apply(preprocess_text_full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIlbVIY_XnjY"
      },
      "outputs": [],
      "source": [
        "posts_split = df['posts'].str.split('\\|\\|\\|')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJkkDKynBkY"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoVQ_N74zDDJ"
      },
      "outputs": [],
      "source": [
        "# Preprocess the text data for  traits\n",
        "df['JP'] = df['posts'].apply(lambda x: x[1])\n",
        "df['class'] = df['posts'].apply(lambda x: 0 if x.startswith('J') else 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRQEQ0XJhHoX"
      },
      "source": [
        "### ***Machine Learning***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9lJtwv-unur"
      },
      "source": [
        "### ***Apply POS TAGGING***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8xbX3cpzQFm"
      },
      "outputs": [],
      "source": [
        "# Load the model for spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform POS tagging\n",
        "def pos_tagging(text):\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    return \" \".join(pos_tags)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2UDT_QXxj6l"
      },
      "outputs": [],
      "source": [
        "# Apply POS tagging to the posts\n",
        "df['pos_tags'] = df['posts'].apply(pos_tagging)\n",
        "\n",
        "X_ns = df['pos_tags']\n",
        "y_ns = df['JP']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction for all four traits\n",
        "# Step 2: Feature Extraction\n",
        "posts_combined = posts_split.apply(lambda x: ' '.join(x))\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_ns = tfidf_vectorizer.fit_transform(posts_combined)"
      ],
      "metadata": {
        "id": "zEK0VYIA6Es6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Vhl5OYX7A38G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "sH64bao9BAQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7KVCkc9_8bf"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "X_train_jp, X_test_jp, y_train_jp, y_test_jp = train_test_split(X_jp, y_jp, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_jp = le.fit_transform(y_train_jp)\n",
        "y_test_jp = le.transform(y_test_jp)"
      ],
      "metadata": {
        "id": "hD6axUhqBQda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***ML + EL with POS Tagging***"
      ],
      "metadata": {
        "id": "ApfZuyMQi2Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define models\n",
        "models = {\n",
        "    'SVM': SVC(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': XGBClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression()\n",
        "}"
      ],
      "metadata": {
        "id": "nBX_0M6fBOa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XldsyaG8_fzQ"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate models for a given trait\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, trait_name):\n",
        "    print(f\"Training models for {trait_name}...\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name} for {trait_name}...\")\n",
        "        # Initialize the TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        # Convert POS-tagged text to numerical features for training and testing data\n",
        "        X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "        X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_vectorized, y_train)\n",
        "        y_pred = model.predict(X_test_vectorized)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy for {name} for {trait_name}: {accuracy}\")\n",
        "        print(f\"Classification report for {name} for {trait_name}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"----------------------------------------------------\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDnx0I8xI7Xh"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_model(X_train_jp, X_test_jp, y_train_jp, y_test_jp, \"Judging vs. Perceiving\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTfQjTul-ktJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "# Initialize a figure for the combined ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Initialize arrays to store combined false positive rates and true positive rates\n",
        "all_fpr = np.linspace(0, 1, 100)\n",
        "mean_tpr = 0.0\n",
        "\n",
        "\n",
        "# Plot ROC curve for each classifier and calculate the mean true positive rate\n",
        "for name, model in models.items():\n",
        "    # Fit the model\n",
        "    model.fit(X_train_vectorized, y_train_jp)\n",
        "\n",
        "    # Get scores (decision function output) on the test set\n",
        "    if hasattr(model, \"decision_function\"):\n",
        "        scores = model.decision_function(X_test_vectorized)\n",
        "    else:\n",
        "        scores = model.predict(X_test_vectorized)\n",
        "\n",
        "    # Convert scores into probabilities\n",
        "    y_pred_proba = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "    # Compute ROC curve and ROC area for Introversion vs. Extroversion (IE) trait\n",
        "    fpr, tpr, _ = roc_curve(y_test_njp, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve for the model\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Calculate mean true positive rate\n",
        "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "\n",
        "\n",
        "# Calculate the mean true positive rate across all classifiers\n",
        "mean_tpr /= len(models)\n",
        "mean_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "    # Plot the combined ROC curve\n",
        "plt.plot(all_fpr, mean_tpr, color='black', linestyle='--', lw=2, label=f'Combined ROC (AUC = {mean_auc:.2f})')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Combined Receiver Operating Characteristic (ROC) Curve for J/P POS tagging')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize Naive Bayes classifier\n",
        "model = MultinomialNB()"
      ],
      "metadata": {
        "id": "87B723r_5pL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_jp = accuracy_score(y_test_ns, y_pred_jp)\n",
        "\n",
        "print(\"\\Judging vs. Perceing:\")\n",
        "print(f\"Accuracy: {accuracy_jp}\")\n",
        "print(classification_report(y_test_jp, y_pred_jp))"
      ],
      "metadata": {
        "id": "YMUTChqC5yyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train and evaluate models for a given trait\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, trait_name):\n",
        "    print(f\"Training models for {trait_name}...\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name} for {trait_name}...\")\n",
        "        # Initialize the TfidfVectorizer\n",
        "        vectorizer = TfidfVectorizer(max_features=1000)\n",
        "        # Convert POS-tagged text to numerical features for training and testing data\n",
        "        X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "        X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "        model.fit(X_train_vectorized, y_train)\n",
        "        y_pred = model.predict(X_test_vectorized)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy for {name} for {trait_name}: {accuracy}\")\n",
        "        print(f\"Classification report for {name} for {trait_name}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"----------------------------------------------------\")"
      ],
      "metadata": {
        "id": "fKgrvkFD5loX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVHw93fiutyF"
      },
      "source": [
        "# ***ML + EL with TF-IDF***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a893c57"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL0tT7VxXh8p"
      },
      "outputs": [],
      "source": [
        "# Feature extraction for all four traits\n",
        "# Step 2: Feature Extraction\n",
        "posts_combined = posts_split.apply(lambda x: ' '.join(x))\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_jp = tfidf_vectorizer.fit_transform(posts_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3eUSA88aotz"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_jp = le.fit_transform(y_train_jp)\n",
        "y_test_jp = le.transform(y_test_jp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrvCQNleYGfZ"
      },
      "outputs": [],
      "source": [
        "# Define models\n",
        "models = {\n",
        "\n",
        "    'SVM': SVC(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'XGBoost': xgb.XGBClassifier(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Logistic Regression': LogisticRegression()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCgDYOd7YJyJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# List to store accuracy scores for all models across four traits\n",
        "accuracy_scores = []\n",
        "\n",
        "# Function to train and evaluate models for a given trait\n",
        "def train_and_evaluate_model(X_train, X_test, y_train, y_test, trait_name):\n",
        "    print(f\"Training models for {trait_name}...\")\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name} for {trait_name}...\")\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy for {name} for {trait_name}: {accuracy}\")\n",
        "        print(f\"Classification report for {name} for {trait_name}:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(\"----------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ8zDt4cbdZR"
      },
      "outputs": [],
      "source": [
        "train_and_evaluate_model(X_train_jp, X_test_jp, y_train_jp, y_test_jp, \"Judging vs. Perceiving\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zu7dOlUD1IA2"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSrxO5Bh1N8_"
      },
      "outputs": [],
      "source": [
        "for model in models.values():\n",
        "    model.probability = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzLAAMrUslgK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "\n",
        "# Initialize a figure for the combined ROC curve\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Initialize arrays to store combined false positive rates and true positive rates\n",
        "all_fpr = np.linspace(0, 1, 100)\n",
        "mean_tpr = 0.0\n",
        "\n",
        "# Plot ROC curve for each classifier and calculate the mean true positive rate\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_jp, y_train_jp)\n",
        "    y_score = model.predict_proba(X_test_jp)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test_jp, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "    mean_tpr += np.interp(all_fpr, fpr, tpr)\n",
        "\n",
        "# Calculate the mean true positive rate across all classifiers\n",
        "mean_tpr /= len(models)\n",
        "mean_auc = auc(all_fpr, mean_tpr)\n",
        "\n",
        "# Plot the combined ROC curve\n",
        "plt.plot(all_fpr, mean_tpr, color='black', linestyle='--', lw=2, label=f'Combined ROC (AUC = {mean_auc:.2f})')\n",
        "\n",
        "# Add labels and legend\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Combined Receiver Operating Characteristic (ROC) Curve- J/P TF-IDF ')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "# Show plot\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9jdxp7fvbRd"
      },
      "source": [
        "### ***EDA***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATGOsszIn-CB"
      },
      "outputs": [],
      "source": [
        "df['full_preprocessed_text'] = df['posts'].apply(preprocess_text_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJiXjT20GvDN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Filter the dataset for labels 'J' and 'P'\n",
        "df['label_JP'] = df['type'].apply(lambda x: 'J' if 'J' in x else ('P' if 'P' in x else np.nan))\n",
        "data_JP = df.dropna(subset=['label_JP'])\n",
        "\n",
        "# Text length analysis for labels 'J' and 'P'\n",
        "data_JP['post_length'] = data_NS['full_preprocessed_text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Plot the distribution of post length by type\n",
        "sns.histplot(data=data_JP, x='post_length', hue='label_JP', bins=50, kde=True)\n",
        "plt.title('Distribution of Post Length by Type (J vs P)')\n",
        "plt.xlabel('Post Length')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B-ZSTqOToo6I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "import spacy\n",
        "!pip install\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/mbti_1.csv\")\n",
        "\n",
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load Spacy's English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Combine NLTK and sklearn stopwords\n",
        "stop_words = set(stopwords.words('english')).union(set(ENGLISH_STOP_WORDS))\n",
        "\n",
        "# Filter posts related to 'N' and 'S' labels\n",
        "j_posts = data[data['type'].str.contains('J')]['posts']\n",
        "p_posts = data[data['type'].str.contains('P')]['posts']\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Preprocess the posts\n",
        "data['cleaned_posts'] = data['posts'].apply(preprocess_text)\n",
        "j_posts_cleaned = j_posts.apply(preprocess_text)\n",
        "p_posts_cleaned = p_posts.apply(preprocess_text)\n",
        "\n",
        "# Basic data inspection\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "\n",
        "# Text length analysis\n",
        "data['post_length'] = data['cleaned_posts'].apply(lambda x: len(x.split()))\n",
        "sns.histplot(data=data, x='post_length', hue='type', bins=50, kde=True)\n",
        "plt.title('Distribution of Post Length by Type')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZvGH6xx392s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/mbti_1.csv\")\n",
        "\n",
        "# Combine NLTK and sklearn stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Preprocess the posts\n",
        "data['cleaned_posts'] = data['posts'].apply(preprocess_text)\n",
        "\n",
        "# Filter posts related to 'J' and 'P' labels\n",
        "j_posts_cleaned = data[data['type'].str.contains('J')]['cleaned_posts']\n",
        "p_posts_cleaned = data[data['type'].str.contains('P')]['cleaned_posts']\n",
        "\n",
        "# Generate word clouds\n",
        "def create_wordcloud(text_data, title):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(\" \".join(text_data))\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(title, fontsize=18)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Create word clouds for 'J' and 'P' posts\n",
        "create_wordcloud(n_posts_cleaned, 'Word Cloud for Judging (J) Posts')\n",
        "create_wordcloud(s_posts_cleaned, 'Word Cloud for Perceiving (P) Posts')\n"
      ],
      "metadata": {
        "id": "VVFTlLvt4K2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter posts related to 'J' and 'P' labels\n",
        "JP_posts_cleaned = data[data['type'].str.contains('J|P')]['cleaned_posts']\n",
        "\n",
        "# Combine all posts into a single string\n",
        "combined_text_jp = \" \".join(JP_posts_cleaned)\n",
        "\n",
        "# Generate and display the word cloud\n",
        "wordcloud_jp = WordCloud(width=800, height=400, background_color='white').generate(combined_text_jp)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud_jp, interpolation='bilinear')\n",
        "plt.title('Combined Word Cloud for Judging (J) and Perceiving (P) Posts', fontsize=18)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wy2Plsb94zkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***DL MODELS***"
      ],
      "metadata": {
        "id": "IEGjMkm55EAc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1xpR9mWSJbn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from gensim.models import Word2Vec\n",
        "from torchtext.vocab import GloVe\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import numpy as np\n",
        "import spacy\n",
        "import tqdm\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import nltk\n",
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier # This is where CatBoostClassifier is defined\n",
        "\n",
        "# Download NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier # Remove CatBoostClassifier from here\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.svm import SVC\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "Gp8eLrPZSpte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, GRU, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, SimpleRNN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "from gensim.models.fasttext import FastText\n",
        "import transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout\n",
        "from torchtext.vocab import GloVe\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n"
      ],
      "metadata": {
        "id": "SqxuzTToSkQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "585125e1"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2_ET-kdSJbs"
      },
      "outputs": [],
      "source": [
        "# Preprocess text (simple preprocessing considering only removal of URLs and lowercasing)\n",
        "data['posts'] = data['posts'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
        "data['posts'] = data['posts'].str.lower()\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['posts'], data['class'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split\n",
        "X_train_jp, X_test_jp, y_train_, y_test_jp = train_test_split(X_jp, y_jp, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CZ8ovF0rCRq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9ROoV67SJbt"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train_jp)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_jp)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_jp)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=100)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inh-EzdcSJbt"
      },
      "outputs": [],
      "source": [
        "# Train a Word2Vec model\n",
        "sentences = [sentence.split() for sentence in X_train_jp]\n",
        "w2v_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create an embedding matrix\n",
        "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = w2v_model.wv[word] if word in w2v_model.wv else None\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiIH6bp5SJbu"
      },
      "outputs": [],
      "source": [
        "# Load the GloVe model\n",
        "glove = GloVe(name='6B', dim=100)\n",
        "\n",
        "# Create an embedding matrix for Glove\n",
        "embedding_matrix_glove = np.zeros((len(tokenizer.word_index) + 1, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = glove[word]\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_glove[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train_jp))\n",
        "print(type(X_test_jp))\n",
        "X_train_jp = X_train_jp.tolist()\n",
        "X_test_jp = X_test_jp.tolist()\n"
      ],
      "metadata": {
        "id": "YIbhrMH-cfwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr_sgKQdSJbu"
      },
      "outputs": [],
      "source": [
        "# Initialize Sentence Transformer Model\n",
        "sbert_model = SentenceTransformer('bert-large-nli-mean-tokens')\n",
        "\n",
        "# Encode sentences (for simplification we use mean pooling of embeddings)\n",
        "X_train_embeddings = sbert_model.encode(X_train_jp, show_progress_bar=True)\n",
        "X_test_embeddings = sbert_model.encode(X_test_jp, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orBqbuOxSJbv"
      },
      "outputs": [],
      "source": [
        "def build_model(embedding_matrix, lstm_type='lstm'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=embedding_matrix.shape[0], output_dim=100,\n",
        "                        weights=[embedding_matrix], trainable=False))\n",
        "    if lstm_type == 'lstm':\n",
        "        model.add(LSTM(100))\n",
        "    elif lstm_type == 'bilstm':\n",
        "        model.add(Bidirectional(LSTM(100)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data types of your target variable\n",
        "print(y_train_jp.dtype)\n",
        "\n",
        "# Check for string values in your features (after padding)\n",
        "print(np.unique(X_train_pad))\n",
        "\n",
        "# If you find string values, you need to convert them to numerical representations.\n",
        "# For example, if the padding token is a string, you can convert it to an integer:\n",
        "\n",
        "X_train_pad = np.where(X_train_pad == '512', 0, X_train_pad).astype(np.float32)\n",
        "X_test_pad = np.where(X_test_pad == '512', 0, X_test_pad).astype(np.float32)\n",
        "\n",
        "# If your target variable contains strings, you need to encode them numerically (e.g., using label encoding or one-hot encoding)"
      ],
      "metadata": {
        "id": "dKgCuqUlEllS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_p24a_USJbv"
      },
      "outputs": [],
      "source": [
        "# LSTM Model\n",
        "model_w2v_lstm = build_model(embedding_matrix, 'lstm')\n",
        "model_w2v_lstm.fit(X_train_pad, y_train_jp, epochs=100, batch_size=64, validation_data=(X_test_pad, y_test_jp))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn.metrics\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "ro5m95hs9SRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities for each class for the test data using the trained LSTM model\n",
        "y_pred_probs_lstm = model_w2v_lstm.predict(X_test_pad)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred_lstm = (y_pred_probs_lstm > 0.5).astype(int)\n",
        "\n",
        "# Generate and print the classification report\n",
        "classification_report_lstm = classification_report(y_test, y_pred_lstm)\n",
        "print(\"Classification Report for LSTM Model:\")\n",
        "print(classification_report_lstm)\n"
      ],
      "metadata": {
        "id": "dd1osIXs495F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdUxqGhoIYdX"
      },
      "outputs": [],
      "source": [
        "# Bi-LSTM Model\n",
        "model_w2v_bilstm = build_model(embedding_matrix, 'bilstm')\n",
        "model_w2v_bilstm.fit(X_train_pad, y_train, epochs=100, batch_size=64, validation_data=(X_test_pad, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities for each class for the test data using the trained Bi-LSTM model\n",
        "y_pred_probs_bilstm = model_w2v_bilstm.predict(X_test_pad)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred_bilstm = (y_pred_probs_bilstm > 0.5).astype(int)\n",
        "\n",
        "# Generate and print the classification report\n",
        "classification_report_bilstm = classification_report(y_test, y_pred_bilstm)\n",
        "print(\"Classification Report for Bi-LSTM Model:\")\n",
        "print(classification_report_bilstm)\n"
      ],
      "metadata": {
        "id": "POMJduBT5I1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# lstm Model with GloVe\n",
        "lstm_model_glove = build_lstm_model(embedding_matrix_glove)\n",
        "lstm_model_glove.fit(X_train_pad, y_train, epochs=100, batch_size=64, validation_data=(X_test_pad, y_test))\n"
      ],
      "metadata": {
        "id": "zzou5jbL-jZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Predict probabilities for each class for the test data using the trained  model\n",
        "y_pred_probs = lstm_model_glove.predict(X_test_pad)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Generate and print the classification report\n",
        "classification_report_lstm = classification_report(y_test, y_pred)\n",
        "print(\"Classification Report for CNN Model with GloVe:\")\n",
        "print(classification_report_lstm)\n"
      ],
      "metadata": {
        "id": "R0SW6Rpe_27T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lstm Model with Sentence Embeddings\n",
        "lstm_model_sentence = build_lstm_model(None)  # No embedding layer needed\n",
        "lstm_model_sentence.fit(X_train_embeddings, y_train, epochs=199, batch_size=64, validation_data=(X_test_embeddings, y_test))"
      ],
      "metadata": {
        "id": "OHg_htPV7v8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***LLM-based Model - BERT Large***"
      ],
      "metadata": {
        "id": "Ss_rhRCVFZHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load Dataset (Replace with actual dataset)\n",
        "df = pd.read_csv(\"conscientiousness_dataset.csv\")\n",
        "texts = df['text'].tolist()\n",
        "labels = [0 if label == 'P' else 1 for label in df['label'].tolist()]  # 0: Perceiving (P), 1: Judging (J)\n",
        "\n",
        "# Load BERT-Large Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
        "\n",
        "# Tokenize Data\n",
        "class PersonalityDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encoding = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Split Data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Dataset\n",
        "train_dataset = PersonalityDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = PersonalityDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "# Define BERT-Large Model for J/P Prediction\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-large-uncased')\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(self.bert.config.hidden_size, 2)  # 2 classes (J/P)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        return self.fc(x)\n",
        "\n",
        "# Initialize Model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertClassifier().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training Function\n",
        "def train(model, dataloader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Evaluation Function\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions, true_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask, labels = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['label'].to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "    return accuracy_score(true_labels, predictions)\n",
        "\n",
        "# Train Model\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    train_loss = train(model, train_loader)\n",
        "    val_acc = evaluate(model, val_loader)\n",
        "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Validation Accuracy = {val_acc:.4f}\")\n",
        "\n",
        "# Save Model\n",
        "torch.save(model.state_dict(), \"bert_large_conscientiousness_JP_model.pth\")\n"
      ],
      "metadata": {
        "id": "I1sQb1zaWR_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}